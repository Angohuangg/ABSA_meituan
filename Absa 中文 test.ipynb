{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80189736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86134\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    " \n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    " \n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    " \n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    " \n",
    "rcParams['figure.figsize'] = 12, 8\n",
    " \n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c894552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"C:/Users/86134/Desktop/毕业设计/data/ASAP_ASPECT/ASAP_ASPECT/train.tsv\"\n",
    "df = pd.read_csv(train_data, header=0,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c3896b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213371, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d17ded8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>AspectTerm</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#  大众点评网霸王餐活动#，第一次去布凡面包店，先用百度地图搜一下，从院士路小路进去很好找...</td>\n",
       "      <td>Ambience#Space</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#  大众点评网霸王餐活动#，第一次去布凡面包店，先用百度地图搜一下，从院士路小路进去很好找...</td>\n",
       "      <td>Food#Taste</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#  大众点评网霸王餐活动#，第一次去布凡面包店，先用百度地图搜一下，从院士路小路进去很好找...</td>\n",
       "      <td>Price#Cost_effective</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#  大众点评网霸王餐活动#，第一次去布凡面包店，先用百度地图搜一下，从院士路小路进去很好找...</td>\n",
       "      <td>Price#Discount</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#  大众点评网霸王餐活动#，第一次去布凡面包店，先用百度地图搜一下，从院士路小路进去很好找...</td>\n",
       "      <td>Service#Hospitality</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>####和朋友约在皇庭广场见面，差不多每个星期都会过来这边闲逛，里面大多数餐厅已经吃过了，朋...</td>\n",
       "      <td>Ambience#Noise</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>####和朋友约在皇庭广场见面，差不多每个星期都会过来这边闲逛，里面大多数餐厅已经吃过了，朋...</td>\n",
       "      <td>Ambience#Space</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>####和朋友约在皇庭广场见面，差不多每个星期都会过来这边闲逛，里面大多数餐厅已经吃过了，朋...</td>\n",
       "      <td>Food#Portion</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>####和朋友约在皇庭广场见面，差不多每个星期都会过来这边闲逛，里面大多数餐厅已经吃过了，朋...</td>\n",
       "      <td>Food#Taste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>####和朋友约在皇庭广场见面，差不多每个星期都会过来这边闲逛，里面大多数餐厅已经吃过了，朋...</td>\n",
       "      <td>Price#Discount</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content            AspectTerm  \\\n",
       "0  #  大众点评网霸王餐活动#，第一次去布凡面包店，先用百度地图搜一下，从院士路小路进去很好找...        Ambience#Space   \n",
       "1  #  大众点评网霸王餐活动#，第一次去布凡面包店，先用百度地图搜一下，从院士路小路进去很好找...            Food#Taste   \n",
       "2  #  大众点评网霸王餐活动#，第一次去布凡面包店，先用百度地图搜一下，从院士路小路进去很好找...  Price#Cost_effective   \n",
       "3  #  大众点评网霸王餐活动#，第一次去布凡面包店，先用百度地图搜一下，从院士路小路进去很好找...        Price#Discount   \n",
       "4  #  大众点评网霸王餐活动#，第一次去布凡面包店，先用百度地图搜一下，从院士路小路进去很好找...   Service#Hospitality   \n",
       "5  ####和朋友约在皇庭广场见面，差不多每个星期都会过来这边闲逛，里面大多数餐厅已经吃过了，朋...        Ambience#Noise   \n",
       "6  ####和朋友约在皇庭广场见面，差不多每个星期都会过来这边闲逛，里面大多数餐厅已经吃过了，朋...        Ambience#Space   \n",
       "7  ####和朋友约在皇庭广场见面，差不多每个星期都会过来这边闲逛，里面大多数餐厅已经吃过了，朋...          Food#Portion   \n",
       "8  ####和朋友约在皇庭广场见面，差不多每个星期都会过来这边闲逛，里面大多数餐厅已经吃过了，朋...            Food#Taste   \n",
       "9  ####和朋友约在皇庭广场见面，差不多每个星期都会过来这边闲逛，里面大多数餐厅已经吃过了，朋...        Price#Discount   \n",
       "\n",
       "   polarity  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "5        -1  \n",
       "6         0  \n",
       "7         0  \n",
       "8         0  \n",
       "9         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2d4d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 213371 entries, 0 to 213370\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   content     213371 non-null  object\n",
      " 1   AspectTerm  213371 non-null  object\n",
      " 2   polarity    213371 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360e70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment(polarity):\n",
    "    polarity = int(polarity)\n",
    "    if polarity == -1:\n",
    "        return 0\n",
    "    elif polarity == 0:\n",
    "        return 1\n",
    "    elif polarity == 1:\n",
    "        return 2\n",
    "\n",
    " \n",
    "df['sentiment'] = df.polarity.apply(to_sentiment)\n",
    " \n",
    "class_names = ['negative','neutral', 'positive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24583d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME= 'bert-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b85c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27345cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "551dd280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    " \n",
    "    def __init__(self, reviews, review_aspects, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.review_aspects = review_aspects\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    " \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        review_aspects = str(self.review_aspects[item])\n",
    "        target = self.targets[item]\n",
    "        mask_location = review + \"[SEP]\" + review_aspects\n",
    "        #sequence = review + \"[SEP]\" +review_aspects + \"[MASK]\"\n",
    "        sequence =\"[MASK]\"+review_aspects+\"[SEP]\"+ review ##把mask放到最前面\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sequence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        id_list = len(sequence)\n",
    " \n",
    "        return {\n",
    "            'review_text':sequence,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.int64),\n",
    "            'id_list':id_list\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09aa5667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((192033, 4), (10669, 4), (10669, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "df_val, df_test = train_test_split(\n",
    "    df_test,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    " \n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525aa5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GPReviewDataset(\n",
    "        reviews=df.content.to_numpy(),\n",
    "        review_aspects = df.AspectTerm.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    " \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )\n",
    " \n",
    "BATCH_SIZE = 2\n",
    " \n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2b921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\86134\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets', 'id_list'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcf1f0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)\n",
    "print(data['id_list'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc3f4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([287, 299])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['id_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d4c25ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74f2ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    " \n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        \n",
    " \n",
    "    def forward(self, input_ids, attention_mask,id_list):\n",
    "        last_hidden_state,_,_,_ = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.drop(last_hidden_state)\n",
    "        #data['id_list']\n",
    "        real_output = output[:,1, :]\n",
    "        #output = output[id_list,:]\n",
    "        #output = self.classifier(output)\n",
    "        #return output\n",
    "        return self.out(real_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a78006d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb533a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = data['input_ids'].to(device)\n",
    "#attention_mask = data['attention_mask'].to(device)\n",
    " \n",
    "#print(input_ids.shape) # batch size x seq length\n",
    "#print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f006e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.functional.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70983878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86134\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    " \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    " \n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    " \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47d27ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    n_examples\n",
    "  ):\n",
    "    model = model.train()\n",
    " \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    " \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        #input_ids = input_ids.squeeze(0)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        #attention_mask = attention_mask.squeeze(0)\n",
    "        #print(input_ids.shape) # batch size x seq length\n",
    "        #print(attention_mask.shape) # batch size x seq length\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        id_list = d[\"id_list\"].to(device)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            id_list=id_list\n",
    "        )\n",
    " \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    " \n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    " \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80fd7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    " \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            id_list = d[\"id_list\"].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                id_list=id_list\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    " \n",
    "            loss = loss_fn(outputs, targets)\n",
    " \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    " \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15680c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    " \n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    " \n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    " \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    " \n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    " \n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    " \n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    " \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    " \n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f370c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 160])\n",
      "torch.Size([4, 160])\n"
     ]
    }
   ],
   "source": [
    "##没有什么用的分割线\n",
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    " \n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5ac37e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.7008, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.8497, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.9025, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.9326, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.9566, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.9717, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.9811, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.9840, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.9868, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.9887, device='cuda:0', dtype=torch.float64)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['train_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c44972c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.7119, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7458, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7712, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7712, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7712, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7881, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7797, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7797, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7712, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7712, device='cuda:0', dtype=torch.float64)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d09c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#调用训练好的模型\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18548b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8220338983050848"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "    model,\n",
    "    test_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(df_test)\n",
    ")\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09be1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    " \n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    " \n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            id_list = d[\"id_list\"].to(device)\n",
    " \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                id_list=id_list\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    " \n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    " \n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ec6652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model,\n",
    "    test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32d66e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]user interface[SEP]The minute you fire it up it's all good, very\n",
      "easy user interface.\n",
      "\n",
      "True sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    " \n",
    "review_text = y_review_texts[idx]\n",
    "true_sentiment = y_test[idx]\n",
    "pred_df = pd.DataFrame({\n",
    "    'class_names': class_names,\n",
    "    'values': y_pred_probs[idx]\n",
    "})\n",
    " \n",
    "print(\"\\n\".join(wrap(review_text)))\n",
    "print()\n",
    "print(f'True sentiment: {class_names[true_sentiment]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e599f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ee482ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = \"Hate you!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ad8856c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86134\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd4a26d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: Hate you!!!\n",
      "Sentiment  : negative\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_review['input_ids'].to(device)\n",
    "attention_mask = encoded_review['attention_mask'].to(device)\n",
    " \n",
    "output = model(input_ids, attention_mask)\n",
    "_, prediction = torch.max(output, dim=1)\n",
    " \n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {class_names[prediction]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced400d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_GPU",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
